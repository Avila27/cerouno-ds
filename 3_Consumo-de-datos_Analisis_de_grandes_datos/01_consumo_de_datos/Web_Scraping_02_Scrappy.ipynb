{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrappy\n",
    "\n",
    "\n",
    "![](https://datonauts.com/wp-content/uploads/2016/10/scrapy_architecture.png)\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "## How does Scrapy compare to BeautifulSoup or lxml?\n",
    "\n",
    "BeautifulSoup and lxml are libraries for parsing HTML and XML. Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\n",
    "\n",
    "Scrapy provides a built-in mechanism for extracting data (called selectors) but you can easily use BeautifulSoup (or lxml) instead, if you feel more comfortable working with them. After all, they’re just parsing libraries which can be imported and used from any Python code.\n",
    "\n",
    "In other words, comparing BeautifulSoup (or lxml) to Scrapy is like comparing jinja2 to Django.\n",
    "\n",
    "## Can I use Scrapy with BeautifulSoup?\n",
    "\n",
    "Yes, you can. As mentioned above, BeautifulSoup can be used for parsing HTML responses in Scrapy callbacks. You just have to feed the response’s body into a BeautifulSoup object and extract whatever data you need from it.\n",
    "\n",
    "Here’s an example spider using BeautifulSoup API, with lxml as the HTML parser:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = \"example\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = (\n",
    "        'http://www.example.com/',\n",
    "    )\n",
    "\n",
    "    def parse(self, response):\n",
    "        # use lxml to get decent HTML parsing speed\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        yield {\n",
    "            \"url\": response.url,\n",
    "            \"title\": soup.h1.string\n",
    "        }\n",
    "\n",
    "```\n",
    "\n",
    "### Starting with Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider!!\n",
    "\n",
    "\n",
    "Guardar el siguiente codigo como *quotes_spider.py* en *tutorial/spiders/*\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# start: quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)\n",
    "\n",
    "# End: quotes_spider.py\n",
    "```\n",
    "\n",
    "**Que objetos y funciones se definen y cual es su objetivo?**\n",
    "\n",
    "* name\n",
    "* start_requests()\n",
    "* parse()\n",
    "\n",
    "## Ejecutando nuestro spider\n",
    "```bash\n",
    "\n",
    "$ cd tutorial\n",
    "$ scrapy crawl quotes\n",
    "\n",
    "...\n",
    "2018-04-06 17:41:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2018-04-06 17:41:34 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2018-04-06 17:41:34 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2018-04-06 17:41:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 678,\n",
    " 'downloader/request_count': 3,\n",
    " 'downloader/request_method_count/GET': 3,\n",
    " 'downloader/response_bytes': 5976,\n",
    " 'downloader/response_count': 3,\n",
    " 'downloader/response_status_count/200': 2,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2018, 4, 6, 22, 41, 34, 688923),\n",
    " 'log_count/DEBUG': 6,\n",
    " 'log_count/INFO': 7,\n",
    " 'memusage/max': 54030336,\n",
    " 'memusage/startup': 54030336,\n",
    " 'response_received_count': 3,\n",
    " 'scheduler/dequeued': 2,\n",
    " 'scheduler/dequeued/memory': 2,\n",
    " 'scheduler/enqueued': 2,\n",
    " 'scheduler/enqueued/memory': 2,\n",
    " 'start_time': datetime.datetime(2018, 4, 6, 22, 41, 33, 856990)}\n",
    "2018-04-06 17:41:34 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat tutorial/quotes-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```$ scrapy shell 'http://quotes.toscrape.com/page/1/'```\n",
    "\n",
    "\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "response.css('title') \n",
    "\n",
    "\n",
    "response.css('title::text')[0].extract()\n",
    "\n",
    "\n",
    "response.css(\"div.quote\")\n",
    "\n",
    "\n",
    "quote = response.css(\"div.quote\")[0]\n",
    "\n",
    "\n",
    "title = quote.css(\"span.text::text\").extract_first()\n",
    "\n",
    "\n",
    "author = quote.css(\"small.author::text\").extract_first()\n",
    "\n",
    "\n",
    "tags = quote.css(\"div.tags a.tag::text\").extract()\n",
    "\n",
    "```\n",
    "\n",
    "## Data+Spider\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# start: quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "# end: quotes_spider.py\n",
    "```\n",
    "\n",
    "\n",
    "## Scraped data store\n",
    "\n",
    "\n",
    "```scrapy crawl quotes -o quotes.json```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
