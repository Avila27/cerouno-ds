{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Topics de Twitter con Spark Streaming\n",
    "\n",
    "\n",
    "\n",
    "- Este código no tiene la calidad requerida para sistemas en producción\n",
    "- El Broker de Twitter (*read.py*) usa la librería *socketstream* y queda a la escucha en un socket TCP (como servidor). El script a su vez está filtrando los tweets provenientes de la Streaming API de Twitter: https://dev.twitter.com/streaming/overview\n",
    "- A partir de este momento, el broker puede ser ejecutado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencias y configuración de contextos\n",
    "\n",
    "Cargar las librerías para el contexto: de Spark, SQL y Streaming. Así como otras dependencias del ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "from collections import namedtuple\n",
    "# Namedtuple: https://pymotw.com/2/collections/namedtuple.html, \n",
    "# http://stackoverflow.com/questions/2970608/what-are-named-tuples-in-python\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear el contexto de Streaming. Con la configuración: \"TwitterTrend\" como nombre del programa, 10 segundos como intervalo para batch y dos hilos de ejecución.\n",
    "\n",
    "De igual manera, crear la instancia del contexto de SQL en Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"TwitterTrend\")\n",
    "ssc = StreamingContext(sc, 10)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instanciados los contextos, nos conectamos a la fuente de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"localhost\", 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soporte de Window Operations y tuplas para conteo de etiquetas\n",
    "\n",
    "Creamos una ventana de tiempo.\n",
    "\n",
    "El RDD obtenido del DStream se creará cada 10 segundos, pero los datos que estarán en el RDD serán por los últimos 20 segundos.\n",
    "\n",
    "[Spark Streaming: Window Operations](https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations)\n",
    "\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-dstream-window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto namedtuple para almacenar las etiquetas y sus conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Tweet"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar el flujo de datos\n",
    "\n",
    "1. Cada tweet/mensaje que se recibe, será cortado en palabras (*flatMap()*),\n",
    "2. Por cada registro generado, se filtrarán  las palabras que empiezan con el simbolo **#**,\n",
    "3. Después se convertirán esos registros a minúsculas, \n",
    "4. Se ejecuta una acción de mapeo a (palabra, 1)\n",
    "5. Luego, se reducen los pares (llave, valor) y cuentan las ocurrencias de cada Hashtag (palabra, n)\n",
    "6. El flujo se convierte en un Dataframe con las etiquetas resultantes y sus conteos.\n",
    "7. Entonces, se ordenan las etiquetas de forma descendente,\n",
    "8. Y se toman los primeros 10 registos,\n",
    "9. Finalmente, esos 10 registros se almacenan en una tabla temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) # 1\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # 2-3\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # 4 \n",
    "  .reduceByKey( lambda a, b: a + b ) # 5\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # 5,\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort(desc(\"count\")).limit(10).registerTempTable(\"tweets\") ) ) # 6 -7 -8-9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) )\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") )\n",
    "  .map( lambda word: ( word.lower(), 1 ) )\n",
    "  .reduceByKey( lambda a, b: a + b )\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) )\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort(desc(\"count\")).limit(10).registerTempTable(\"tweets\") ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lazy evaluation\n",
    "ssc.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql( 'Select tag, count from tweets' ).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniciar el proceso de Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar las tendencias\n",
    "\n",
    "Podemos cambiar los valores de time.sleep() junto con la operación en ventana de tiempo para actualizaciones más cortas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "for i in range(0,1000):\n",
    "  time.sleep( 2 )\n",
    "  top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "  top_10_df = top_10_tweets.toPandas()\n",
    "  display.clear_output(wait=True)\n",
    "  plt.figure( figsize = ( 10, 8 ) )\n",
    "  sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
